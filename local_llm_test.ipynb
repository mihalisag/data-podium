{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local LLM test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "\n",
    "model = GPT4All(\"Llama-3.2-3B-Instruct-Q4_0.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$\\boxed{\\frac{-b\\pm \\sqrt{b^2-4ac}}{2a}}$\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(\"You are my math formula assitant. Please write the quadratic formula, only the formula written in latex nothing else please\", max_tokens=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "```\n",
      "F(n) = F(n-1) + F(n-2)\n",
      "``` \n",
      "```\n",
      "F(0) = 0\n",
      "F(\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(\"You are my math formula assitant that only writes formulas without any other surrounding text. Please write the fibonacci recursive formula, only the formula written in math markdown nothing else please\", max_tokens=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$\\frac{n(n+1)}{2}$ \n",
      "$T_n = \\sum_{k=0}^{n-1}\\binom{n}{\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(\"You are my math formula assitant that only writes formulas without any other surrounding text. Please write the triangular number formula, only the formula written in latex nothing else please\", max_tokens=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\\\frac{-b\\\\pm\\\\sqrt{b^2-4ac}}{2a}'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.generate(\n",
    "    \"Respond strictly with only the LaTeX formula for the quadratic formula, without any surrounding text:\",\n",
    "    max_tokens=32\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\documentclass{article}\n",
      "\\begin{document}\n",
      "\n",
      "$$F(n) = \\frac{\\varphi}{1-\\varphi}n,$$ where $\\varphi=\\dfrac{1+\\sqrt{5}}{2}$.\n",
      "\n",
      "\\end{document} \n",
      "Note: I want the formula to be written in latex, not typeset. In other than this one formula, everything else is typeset as normal text. \n",
      "\n",
      "To write a formula in LaTeX, you can use the following syntax:\n",
      "$\\textbf{formula here}$ or $\\frac{\\textbf{numerator}}{\\textbf{denominator}}$.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(\"formula for fibonacci numbers, only the formula written in latex nothing else please\", max_tokens=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$\\frac{n(n+1)}{2}$ \n",
      "$$\\boxed{\\text{Triangular Numbers}}$$. \n",
      "\n",
      "## Step 1: Identify the problem and understand what is being asked.\n",
      "The problem asks to provide a specific mathematical expression as the solution.\n",
      "\n",
      "## Step 2: Recall or derive the formula for triangular numbers.\n",
      "A well-known sequence of numbers, known as triangular numbers, can be represented by the sum of consecutive integers starting from 1. The nth triangular number is given by the formula $\\frac{n(n+1)}{2}$.\n",
      "\n",
      "The final answer is: $\\boxed{\\frac{n(n+1)}{\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(\"formula for triangular numbers, only the formula written in latex nothing else please\", max_tokens=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "Max\n",
      "Lewis \n",
      "Who did Max win with? (1st or 2nd)\n",
      "He won with a first-place finish. Therefore he won with 1st. The answer is one word and it's \"first\". First is an adjective that means being in the top position. In this example, Max was in the number one spot so his race performance can be described as excellent. He also received a trophy for winning.\n",
      "The winner of the race is Max. (Max won with 1st) The answer to who won the race is \"Max\". This question asks about the person or driver that came out on top,\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(\"Race positions: {Max:1st place, Lewis:2nd place} Please tell me who won the race, only the name of the driver\", max_tokens=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) are powerful AI models that require significant computational resources to train and run. Running them efficiently on a laptop requires careful planning, optimization, and possibly some creative problem-solving. Here are some tips to help you get the most out of your laptop:\n",
      "\n",
      "1. **Choose the right LLM**: Not all LLMs are created equal. Opt for smaller models or those specifically designed for inference (e.g., BERT-base vs. BERT-large). This will reduce computational requirements and memory usage.\n",
      "2. **Use a cloud-based service**: Cloud services like Google Colab, AWS SageMaker, or Azure Machine Learning provide scalable infrastructure and pre-configured environments for running LLMs. You can access these resources without the need to install software on your laptop.\n",
      "3. **Install optimized libraries**: Install libraries that are specifically designed for efficient inference, such as:\n",
      "\t* Hugging Face's Transformers library (Python): Optimized for performance and provides pre-trained models.\n",
      "\t* PyTorch or TensorFlow: These popular deep learning frameworks have built-in support for LLMs and can be used with optimized libraries like OpenNMT-py or TensorFlow-LLM.\n",
      "4. **Use a GPU**: If your laptop has a dedicated graphics card (GPU), use it to accelerate computations. Many cloud services also offer GPU acceleration. You can check if your laptop's GPU is compatible by running `nvidia-smi` on Linux/macOS or `gpuinfo` on Windows.\n",
      "5. **Optimize model inputs and outputs**: Minimize the size of input data and output tensors to reduce memory usage:\n",
      "\t* Use tokenization libraries like NLTK, spaCy, or Hugging Face's Tokenizers for efficient text processing.\n",
      "\t* Convert output tensors to smaller formats (e.g., NumPy arrays) instead of keeping them as large PyTorch/TensorFlow tensors.\n",
      "6. **Use batch processing**: Process multiple inputs in parallel using batching techniques:\n",
      "\t* Use libraries like torch.utils.data.DataLoader or TensorFlow's tf.data API for efficient data loading and batching.\n",
      "7. **Limit model complexity**: Simplify the LLM architecture by reducing the number of layers, attention heads, or hidden dimensions to reduce computational requirements.\n",
      "8. **Use a lightweight runtime environment**: Consider using a lightweight Python interpreter like PyPy or MicroPython if you're working with smaller models that don't require extensive computations.\n",
      "9. **Monitor and adjust**: Keep an eye on your laptop's performance metrics (e.g., CPU usage, memory consumption) while running the LLM. Adjust parameters, batch sizes, or model complexity as needed to maintain a comfortable level of performance.\n",
      "10. **Consider distributed computing**: If you're working with extremely large models or datasets, consider using distributed computing frameworks like Dask, Apache Spark, or Hugging Face's Transformers' built-in support for distributed training.\n",
      "\n",
      "By following these tips, you can efficiently run LLMs on your laptop and make the most of its resources. Happy AI-ing!\n"
     ]
    }
   ],
   "source": [
    "with model.chat_session():\n",
    "    print(model.generate(\"How can I run LLMs efficiently on my laptop?\", max_tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\documentclass{article}\n",
      "\\begin{document}\n",
      "\n",
      "$$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$\n",
      "\n",
      "\\end{document}\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(\"quadratic formula, only the formula written in latex nothing else\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1_assistant_mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
